{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7858314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('default')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import re #for camel case conversion\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "import re #for camel case conversion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['LDA_name'] = 'num-directstr_thr-0_tn-400'\n",
    "\n",
    "from extract.feature_extraction.topic_features_LDA import extract_topic_features\n",
    "from extract.feature_extraction.sherlock_features import extract_sherlock_features\n",
    "from utils import get_valid_types\n",
    "from model import models_sherlock\n",
    "from model.torchcrf import CRF\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7b0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case(s):\n",
    "  s = re.sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\")\n",
    "  return ''.join([s[0].lower(), s[1:]])\n",
    "\n",
    "def sherlock_case(s):\n",
    "    s = re.sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\")\n",
    "    s = ''.join([s[0].lower(), s[1:]])\n",
    "    s = ''.join(map(lambda x: x if x.islower() else \" \"+x, s))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffd1603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['address', 'affiliate', 'affiliation', 'age', 'album', 'area', 'artist', 'birthDate', 'birthPlace', 'brand', 'capacity', 'category', 'city', 'class', 'classification', 'club', 'code', 'collection', 'command', 'company', 'component', 'continent', 'country', 'county', 'creator', 'credit', 'currency', 'day', 'depth', 'description', 'director', 'duration', 'education', 'elevation', 'family', 'fileSize', 'format', 'gender', 'genre', 'grades', 'isbn', 'industry', 'jockey', 'language', 'location', 'manufacturer', 'name', 'nationality', 'notes', 'operator', 'order', 'organisation', 'origin', 'owner', 'person', 'plays', 'position', 'product', 'publisher', 'range', 'rank', 'ranking', 'region', 'religion', 'requirement', 'result', 'sales', 'service', 'sex', 'species', 'state', 'status', 'symbol', 'team', 'teamName', 'type', 'weight', 'year']\n"
     ]
    }
   ],
   "source": [
    "TYPENAME = os.environ['TYPENAME']\n",
    "valid_types = get_valid_types(TYPENAME)\n",
    "print(valid_types)\n",
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(valid_types)\n",
    "\n",
    "MAX_COL_COUNT = 10\n",
    "topic_dim = 400\n",
    "pre_trained_loc = './pretrained_sato'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(\"PyTorch device={}\".format(device))\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093a0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_cols = {}\n",
    "sherlock_feature_groups = ['char', 'word', 'par', 'rest']\n",
    "for f_g in sherlock_feature_groups:\n",
    "    feature_group_cols[f_g] = list(pd.read_csv(join(os.environ['BASEPATH'],\n",
    "                                          'configs', 'feature_groups', \n",
    "                                          \"{}_col.tsv\".format(f_g)),\n",
    "                                           sep='\\t', header=None, \n",
    "                                           index_col=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8179e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_vec = lambda x: np.pad(x, (0, topic_dim - len(x)),\n",
    "                    'constant',\n",
    "                    constant_values=(0.0, 1/topic_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb98491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(num_tags=78)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = models_sherlock.build_sherlock(sherlock_feature_groups, num_classes=len(valid_types), topic_dim=topic_dim, dropout_ratio=0.35)\n",
    "#classifier.load_state_dict(torch.load(join(pre_trained_loc, 'sherlock_None.pt'), map_location=device))\n",
    "model = CRF(len(valid_types) , batch_first=True).to(device)\n",
    "#model.load_state_dict(torch.load(join(pre_trained_loc, 'model.pt'), map_location=device))\n",
    "\n",
    "loaded_params = torch.load(join(pre_trained_loc, 'model.pt'), map_location=device)\n",
    "classifier.load_state_dict(loaded_params['col_classifier'])\n",
    "model.load_state_dict(loaded_params['CRF_model'])\n",
    "\n",
    "classifier.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c93e730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df):\n",
    "\n",
    "    df_dic = {'df':df, 'locator':'None', 'dataset_id':'None'}\n",
    "    feature_dic = {}\n",
    "    n = df.shape[1]\n",
    "\n",
    "    # topic vectors\n",
    "    topic_features = extract_topic_features(df_dic)\n",
    "    topic_vec = pad_vec(topic_features.loc[0,'table_topic'])\n",
    "    feature_dic['topic'] = torch.FloatTensor(np.vstack((np.tile(topic_vec,(n,1)), np.zeros((MAX_COL_COUNT - n, topic_dim)))))\n",
    "\n",
    "\n",
    "    # sherlock vectors\n",
    "    sherlock_features = extract_sherlock_features(df_dic)\n",
    "    for f_g in feature_group_cols:\n",
    "        temp = sherlock_features[feature_group_cols[f_g]].to_numpy()\n",
    "        temp = np.vstack((temp, np.zeros((MAX_COL_COUNT - n, temp.shape[1])))).astype('float')\n",
    "        temp = np.nan_to_num(temp)\n",
    "        feature_dic[f_g] = torch.FloatTensor(temp)\n",
    "\n",
    "    # dictionary of features, labels, masks\n",
    "    return feature_dic, np.zeros(MAX_COL_COUNT), torch.tensor([1]*n + [0]*(MAX_COL_COUNT-n), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93250f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df):\n",
    "\n",
    "    feature_dic, labels, mask = extract(df)\n",
    "\n",
    "    emissions = classifier(feature_dic).view(1, MAX_COL_COUNT, -1)\n",
    "    mask = mask.view(1, MAX_COL_COUNT)\n",
    "    pred = model.decode(emissions, mask)[0]\n",
    "\n",
    "    return label_enc.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out_true_types = '../../combined/results/true_types'\n",
    "path_out_predictions = '../../combined/results/predictions'\n",
    "path_out_temp = '../results/sato_sato'\n",
    "path_data = '../table_data/sato_tables/all'\n",
    "write_out_limit = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8ff01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n",
      "K1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/senn/virtualenvs/col2type/lib/python3.7/site-packages/pandas/core/strings/object_array.py:90: FutureWarning: Possible nested set at position 1\n",
      "  regex = re.compile(pat, flags=flags)\n",
      "/home/senn/virtualenvs/col2type/lib/python3.7/site-packages/pandas/core/strings/object_array.py:90: FutureWarning: Possible nested set at position 1\n",
      "  regex = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "writing out predictions...\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "writing out predictions...\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "writing out predictions...\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "writing out predictions...\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "writing out predictions...\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "K2\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "writing out predictions...\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "writing out predictions...\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "writing out predictions...\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "writing out predictions...\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "writing out predictions...\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "K0\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "writing out predictions...\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "writing out predictions...\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "writing out predictions...\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "writing out predictions...\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "writing out predictions...\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "K3\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "writing out predictions...\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "writing out predictions...\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "writing out predictions...\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "writing out predictions...\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "writing out predictions...\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "K4\n",
      "writing out predictions...\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "writing out predictions...\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "writing out predictions...\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "writing out predictions...\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "writing out predictions...\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "writing out predictions...\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = [] \n",
    "true_types = []\n",
    "idx = 0\n",
    "write_out_idx = 0\n",
    "#loop over al subdirectories of path_data\n",
    "for subdir in listdir(path_data):\n",
    "    print(subdir)\n",
    "    if(subdir == 'processed'): continue\n",
    "    path_subdir = join(path_data, subdir)\n",
    "    #loop over all files in subdir\n",
    "    for file in listdir(path_subdir):\n",
    "        if(idx % 100 == 0):\n",
    "            print(idx)\n",
    "        idx += 1\n",
    "#         print(file)\n",
    "        path_file = join(path_subdir, file)\n",
    "\n",
    "        df = pd.read_csv(path_file)\n",
    "        df_header = list(df)\n",
    "#         if (len(df_header)>1):\n",
    "#             print('table: '+ str(df_header) )\n",
    "        prediction = evaluate(df)\n",
    "        predicted_labels.extend(prediction)\n",
    "        true_types.extend(df_header)\n",
    "        if (idx % write_out_limit == 0):\n",
    "#             print(predicted_labels)\n",
    "#             print(len(predicted_labels))\n",
    "#             print(true_types)\n",
    "#             print(len(true_types))\n",
    "            # write out predictions to parquet file\n",
    "            print(\"writing out predictions...\")\n",
    "            df_pred = pd.DataFrame(predicted_labels, columns=['predicted_type'])\n",
    "            # write out predictions, format: model_dataset_tableCount_writeOutCount.parquet\n",
    "            df_pred.to_parquet(join(path_out_temp, 'sato_sato_'+ str(idx) + '_' +str(write_out_idx)+ '.parquet'))\n",
    "            write_out_idx += 1\n",
    "            predicted_labels = []\n",
    "            true_types = []\n",
    "            \n",
    "df_pred = pd.DataFrame(predicted_labels, columns=['predicted_type'])\n",
    "df_pred.to_parquet(join(path_out_temp, 'sato_sato_'+ str(idx) + '_' +str(write_out_idx)+ '.parquet'))\n",
    "\n",
    "            \n",
    "            \n",
    "# for idx, col in enumerate(test_values['values']):\n",
    "#     col = ast.literal_eval(col)\n",
    "#     df = pd.DataFrame(columns=['col'], data=col)\n",
    "#     prediction = evaluate(df)\n",
    "#     prediction = str(prediction[0])\n",
    "#     predicted_labels.append(prediction)\n",
    "#     if(idx % 100 == 0):\n",
    "#         print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a67113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'publisher', 'nationality', 'rank', 'order', 'fileSize', 'classification', 'continent', 'birthDate', 'birthPlace', 'format', 'sales', 'weight', 'club', 'jockey', 'company', 'description', 'city', 'isbn', 'name', 'symbol', 'artist', 'notes', 'region', 'elevation', 'collection', 'area', 'component', 'service', 'category', 'organisation', 'requirement', 'country', 'genre', 'result', 'religion', 'family', 'command', 'duration', 'brand', 'team', 'code', 'capacity', 'status', 'ranking', 'affiliate', 'currency', 'affiliation', 'creator', 'education', 'position', 'owner', 'gender', 'type', 'album', 'address', 'year', 'day', 'person', 'language', 'teamName', 'credit', 'grades', 'depth', 'state', 'county', 'industry', 'class', 'operator', 'product', 'sex', 'director', 'origin', 'age', 'plays', 'range', 'location', 'manufacturer', 'species'}\n",
      "            type\n",
      "120604     state\n",
      "120605       age\n",
      "120606  location\n",
      "120607  position\n",
      "120608      name\n"
     ]
    }
   ],
   "source": [
    "files = listdir(path_out_temp)\n",
    "files.sort(key=lambda x: int(x.split('_')[2]))\n",
    "# print(files)\n",
    "df_combined = pd.DataFrame()\n",
    "for file in files:\n",
    "    filepath = os.path.join(path_out_temp, file)\n",
    "    df_partial = pd.read_parquet(filepath)\n",
    "    df_combined = pd.concat([df_combined, df_partial], ignore_index=True, sort=False)\n",
    "\n",
    "df_combined.columns = ['type']\n",
    "# print(set(df_combined['type']))\n",
    "# for val in df_combined['type']:\n",
    "#     if '1' in val: print(val)\n",
    "# df['type'].transform()\n",
    "print(df_combined.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb79fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predicted_labels)\n",
    "# print(true_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fbfefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_parquet(join(path_out_predictions, \"sato_sato.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
