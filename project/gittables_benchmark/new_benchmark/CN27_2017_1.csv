,isbn,publisher,language,type,notes,country,code
0,,,en,,"<p>HCA, hierarchical cluster analysis is adopted to conduct pattern analysis for studying which group of students are most likely to drop out. It is particularly interesting in comparing individual longitudinal pattern with his/her cohort peers' patters.</p> <p>"" students are clustered hierarchically using the average linkage algorithm, while grades are clustered chronologically and by an ordered repeating pattern from core subjects to non-core subjects.""</p> <p>&nbsp;</p> <p>###</p> <p>The HCA analysis adopted here, complements the conventional regression analysis, in presenting the disaggregated analysis, saying that the each single observation in the entire data set can be analyzed. Not providing the overall parameter estimates enabling inference to population, the HCA is more like a unique way of 'look inside' and 'look closer' to our sample, each students' entire longitudinal history of achievement.</p> <p>&nbsp;</p>",,
1,,,en,,"<p>SNA in classroom.</p> <p>Social network analysis and relational data are simple yet profound in conducting researches. First of all, it is fairly simple to conduct SNA, and the visualization of it enables quite easy and straightforward measurement. However, it does imply and provide profound hints way beyond these, providing clues to explore deeper and more detailed research questions. It is a good start to both understand the learning community and the questions researchers are interested in. I particularly think it is a good bridge of quantitative and qualitative study of learning community, instead of peer qualitative interviews/ observations, SNA visualization offers another way to both understand the community better and to extend to the analysis of learning.</p> <p>###</p> <p>The social network analysis has many implementations in classroom analysis, and can be further explored in/as many problems. One potential question I am particularly interested in is 'ties as predictors of performance', which is quite intuitive in the sense that the closeness/tie between students are intuitively expected to be the predictor of study group formation or resources sharing, hence maybe a significant predictor of peer performance, which is an aspect can be further explored in education researches since we do value peer effect a lot and it implies a huge policy implication.</p> <p>&nbsp;</p>",,
2,,,,,"<p>Contextualized profile?</p> <p>The contextualized profile of the performance for individual modelling?, does this profile function the same as human brain? in the sense that it predicts in which environment are the learners and predict the respective personality or any pattern accordingly?</p> <p>&nbsp;</p> <p>&nbsp;</p>; <p>&nbsp;L.Todd Rose refer to the education market as a non-functional market, there is not enough transparency, which links to his point that market should recognize the importance for individual learners to own their own data, while under the current circumstance, every company or institution is just trying to innovate on their own platform, and ""hoarding"" data because that's the business model they have to have under the current market of education innovation/ Ed-tech.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>; <p>There is a shift in the kind of patterns that we look for.</p> <p>In stead of the expected level of population, which is average level, now more on personal patterns across all dimensions.</p> <p>&nbsp;</p>",,
3,,,en,,"<p>Research: Modelling students' changing knowledge state during skill acquisition.</p> <p>Cognitive model building for skill acquisition.</p> <p>Associate each programming action with a single production rule, which enabled by the knowledge tracing process modeling students as an overlay of the production rules and the mastery-based curriculum structure.</p> <p>&nbsp;</p> <p>The correlation of of ln(slip weight), provides the variability of the models' prediction of test performance reflecting persisting differences among students in the slip parameter.</p> <p>Considering students' performance is different from predicting students' knowledge state, which is intuitive in the sense that although students acquired the underlying programming rules, they can get wrong in the quiz.</p>",,
4,978-1-4503-1111-3,ACM,,,"<p>Difference between Educational Data Mining and Learning Analytics:</p> <p>1.EDM is in the tech field; LA is in educational field</p> <p>2. EDM, automation and tech driven; LA use tech as a tool to enhance human judgement</p> <p>3. EDM focuses more on individual learner; LA is more holistic</p>",,
5,,O'Reily Media,,,"<p>Evaluation Metrics for supervised learning models.</p> <p>Accuracy= correct predictions/ total data points.</p> <p>Precision = happy correct answers/ total items returned.</p> <p>Recall = happy correct answers/ total relevant items.</p> <p>AUC, area under ROC curve, shows how may correct positive classifications can be gained as you allow for more and more false positives.</p> <p>ROC provided nuanced details about the behaviours of the classifiers, but it's hard to compare ROC with each other quickly.</p> <p>F1 score = 2 precision *recall/ precision+recall</p> <p>&nbsp;</p>",,
6,,,,,"<p>Reasons:</p> <p>1. Learning is multi-dimentional, and it is being simplified too much for analyzing.&nbsp;</p> <p>2. Learning is broad, it is too contextualized to be measured outside its context.</p> <p>3. Learning in different fields differ, and the availability of tools that can be used to analyze learning also differ across different fields.</p> <p>4. Learning is personal, and the structure is hard to define, and it is the behaviors that can be captured, not necessarily the progress we made in our brain.</p> <p>5. The hard part is how to measure, to find reliable and simple proxies indicators to measure learning. Certain indicators can indicate understanding.</p> <p>6. Learning is complex, a black box, having other cultural and social meanings embedded.</p> <p>7. A measure of competence is not enough, but also to measure at different time points to draw the conclusion of learning.</p> <p>8. Measurement, psychological construct or achievement?</p> <p>9. Analytics, to reveal to the learner the connections that they are making, probably without aware it, reveal more about their connected learning, not just diagnosis.</p> <p>10. Learning might be a collection of things that we don't understand, regarding measuring it, both over-simplication and complexation are problems.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
7,,,,,"<p>Data Mining Bias.</p> <p>In statistics, there is one kind of Type 1 error, the data mining bias, regarding enforced data mining trying to find any correlation or even causality that is actually frauds.</p> <p>And as indicated in the comics, any compulsive finding can be easily manipulated in education policy or intervention, and it is like, what I personally call feeding the students the formula milk powder.</p> <p>SO definitely data scientists should be extra cautious on any conclusion or intervention they suggest, even the most authentic data can tell a fraud if there is bias embedded.</p>",,
8,,,,,<p>Argument of a function is the same as a function call right?</p>,,
9,,ACM,,,"<p>Feedback loop.</p> <p>Closing the feedback loop to improve learning is at the heart of good learning analytic practice, which asks for high quality data, and also the need for and value of human meaning-making to interpret the data for the further step of transforming the data into actionable intelligence to truly achieve the effectiveness of feedback loop.</p> <p>And the sector of learning analytics is widely seen as entailing a feedback loop, where the actionable intelligence is produced from data related to learning, to understand leaning and fundamentally to work out the interventions aiming of improving learning.</p> <p>single-loop learning</p> <p>double-loop learning</p> <p>#Reason: A gap in knowledge and practice between the data about learning and the academics who need to be in a position to act to improve teaching provision: a gap that could easily widen as the quantity and complexity of data increases.</p> <p>#The role of data wranglers: they are not only to analyze the data, but to increase the familiarity of academics with the data sources, to build the learning analytics capacity as part of a Community of Practice.&nbsp;</p> <p>#Both bottoms-up and top-down for understanding learning and providing actionable intelligence: A bottom-up, grounded approach is necessary for sense making, saying that contextualization helps to understand; and meaningful engagement at the strategic, top-down level is also essential, especially regarding organizational changes that learning analytics aiming to achieve.</p> <p>&nbsp;</p>; <p>Human interpreters.</p> <p>1. The need of multidisciplinary teams, when it comes to interpret the data, in which human interpreters play a key role, concluded from previous literature.</p> <p>2. Sense-making and social processes are important, because of the complexity of the data we are dealing with, since learning itself is a complex and social process, and the action and intention to interpret the data definitely asks for the ability to contextualize it in understanding and analyzing it.</p> <p>&nbsp;</p>",,
10,,,,,"<p>How 'personalized' different from individual, learner-centered, or customized learning.</p> <p>Zuckerberg defines it as ""teachers 'working with students to customize instruction to meet the students' individual needs and interest' "", which I still don't think is too different from the prior proposed terminologies, and neither intuitive in understanding. While the underlying principles are using technology, algorithms to provide students with 'personalized' content based on their past behaviour and demonstrated interests. To me, this is as bias as only using historical data to predict stock exchange price, saying that technology is only focusing what the students presented previously, but teachers are more able to use contents and instructions to guide/encourage students to progress. The expectation from teachers, to a large extent, acts an active role in students' development and progress.&nbsp;</p> <p>###</p> <p>It is not practical in using algorithm to predict what is appropriate for each student, in the sense that our education, to a large extent, is not just about personal development, but also more of peer competition. And the fact that the technology is offering what predicted to fit the students, might be complementary for teachers' reference, is definitely not enough in encouraging students to 'step out of their comfort zone'.</p> <p>&nbsp;</p>",,
11,,Youtube,,,"<p>Feature Selection,</p> <p>Knowledge Discovery, for interpretability and for insight: Only a few factors matter in prediction; something in 2D for students to understand, which is less considered in machine learning field than data mining.</p> <p>Curse of Dimensionality: adding features needs more data you have, amount of data = alpha power of N (features). The power of features help us to narrow down to a few features, which hopefully makes the learning process a lot easier.</p> <p>The goal is to use, in general, a bunch of features, and apply algorithm to adjust only a few features, to both understand the data well and have easier learning problems.</p>",,
12,,,,,"<p>It seems like a very useful resources, just I haven't played with R enough to practice everything.</p>",,
13,,,,,"<p>Validity and Reliability</p> <p>Is LA, in fact, more as support technology itself while most of the stakeholders actually use it solely to tell the whole story? Apart from guaranteeing the reliability of the data, the external validity of the conclusion draw from learning analytics seems to be an underlying vulnerability of LA? saying if its representativity is valid to draw conclusion on either learning patterns or a particular education intervention, targeting a student population on a rather large scale.</p> <p>If the answer is NO, then is it one of the reasons for L.Todd Rose to argue that education data should focus, more on individual learning, rather than the expected value of population?</p>",,
14,,,,,"<p>Results:</p> <p>Intention predicts completion.</p> <p>Student knowledge increased.</p> <p>Limited data suggests that face-to-face students learned at least as much as online-only students.</p> <p>Students at all incoming knowledge levels benefited similarly from the course.</p> <p>Students in the programming and concepts tracks had similar gains in concepts knowledge, but programming students gained further knowledge.</p> <p>Normalized knowledge gains are very difficult to predict; measures of relevant effort were strongest.</p> <p>Predicting student end-of-term performance is difficult; appropriate predictor variables may be lacking.</p> <p>Among students who responded to a 5-month follow-up, most student learning gains were retained after 5 months. Programming students retained more than concepts students did; very limited data on face-to-face students suggests their retention is at least as high as that of online-only students.</p> <p>&nbsp;</p>",,
15,,International Educational Data Mining Society,en,,"<p>Skill Models for online courses for better evaluation and course refinement.</p> <p>""Our method assumes that online courses have a pre-defined skill map for which skills are associated with formative assessment items embedded throughout the online course.""</p> <p>The limitation in interpretability lies in the fact that "" the obvious limitation of the current technique is its dependence on manual inspection.""</p> <p>The articles present the analysis for eEPIPHANY outperforming human-crafted skill models, "" eEPIPHANY is an efficient, practical, and quick method to automatically discover skill models from online course data without human interaction.""</p>",,
16,,,,,"<p>Network data vs. conventional data</p> <p>Different from conventional data emphasizing actors and attributes, network data focus more on actors and relations, which leads to the composition of nodes and and edges in network data.</p> <p>&nbsp;</p> <p>###</p> <p>One key point of the network data arose is that since we are interested in both the actors and finding the relations in between different actors, so when we design sample for addressing the population we are interested in, we normally cannot include single observations, with that being said, if one observation is selected, than relevant 'neighbors' and their relations should also be included in the data collection.</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
17,978-0-9952408-0-3,,EN,,"<p>Learning Analytics Dashboards.</p> <p>Dashboards are particularly good regarding visually present the learning traces, process of the learners, which is what learning analytics is essentially for. However, not all the data are good data, and having visualization just for the sake of it is not enough. Though it appears that it is the technique enables dashboard visualization, it is actually the goal set by the developper/evaluator that is important. What is the goal of developing the learning analytis dashboard, the very first step is to understand the goal, this paves the foundation for the actual execution afterwards.</p> <p>&nbsp;</p> <p>###</p> <p>Having dashboard is good for learning analytics, either for analysis or for presenting the result for others. But it is not just done for the sake of it, though it is the case a lot of times. The chapter offered four guidance, regarding what kind of data can be visualized, who is the intended audience, what is the goal of visualization, and how can it be done.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
18,978-0-9952408-0-3,Society for Learning Analytics Research (SoLAR),,,"<p>Psychological Measurement</p> <p>1. Composition: defining a construct; specifying a measurement model and developing a reliable instrument; analyzing and accounting for various sources of error (including operator error); and framing a valid argument for particular uses of the outcome.</p> <p>2. Construct</p> <p>&nbsp;&nbsp;&nbsp; - Construct is used interchangeably with latent variable, while trait is used to imply a construct that is stable over time. (e.g. tape measure provides a scale)</p> <p>&nbsp;&nbsp;&nbsp; - Psychological constructs, e.g. math ability and extraversion, equating the constructs to scores on instruments used to measure them.</p> <p>&nbsp;&nbsp;&nbsp; -We can infer an extremely partial list of constructs relevant to learning analytics from the instruments already developed to measure them.</p> <p>3. Measurement Instruments</p> <p>&nbsp;&nbsp;&nbsp; -tests, questionnaires, having items or indicators.</p> <p>4. Error</p> <p>&nbsp;&nbsp;&nbsp; - random error, unbiased,&nbsp;</p> <p>&nbsp;&nbsp;&nbsp; - systematic, biased</p> <p>5. Models to use</p> <p>&nbsp;&nbsp;&nbsp; - Factor analysis</p> <p>&nbsp;&nbsp;&nbsp; -Latent class and latent mixture models</p> <p>&nbsp;&nbsp;&nbsp; -Item Response Theory</p> <p>6. Explanation and/or Prediction</p> <p>Learning analytics has been described as a middle space between learning science and analytics, so the field may benefit from understanding the nuances of both perspectives.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
19,978-0-9952408-0-3,Society for Learning Analytics Research (SoLAR),,,"<p>Predicting students' academic achievement.</p> <p>Predictive modelling in the educational domain.</p> <p>Different from explanatory modelling, which aims to provide explanations for the results and imply a causation, the predictive modelling intends to create a model predicting the values of new data based on observations. Based on the intuition that training data can be used to predict the value of new data. And the implementation of it in education including identifying students vulnerable in learning outcomes, etc.</p> <p>&nbsp;</p> <p>###</p> <p>Workflow: 1. Problem Identification, 2. Data Collection, 3. Classification and Regression, 4. Feature Selection, 5.Model Building (Linear Regression, Logistic Regression, KNN, Decision Trees, Naive Bayes Classifiers, Bayesian Networks, Support Vector Machines, Neutral Networks, Ensemble Methods) 6. Model Evaluation</p> <p>&nbsp;</p>",,
20,978-0-9952408-0-3,Society for Learning Analytics Research,EN,,"<p>Ethics</p> <p>-Persuasive surveillance, the role and unintended consequences of algorithms.</p> <p>-ethics and privacy as crucial enablers within learning analytics.</p> <p>&nbsp;&nbsp;&nbsp; Is it inevitable, for people to realize the importance of the ethics only after a series of misconduct, or even catastrophic consequences? Just like in any other fields?</p> <p>As in Finance sector, the CFA certificate allocates a huge weight on the ethics, aiming to address the ethics in the finance field, and though the Edtech field can adopt some similar approaches, saying ""inventing"" some kind of certificate exam stressing the importance ethics, but comes to the real world practice, it is the transparency and the regulation of the market increases people's trust on the data users.</p> <p>But then comes to the question that, who and which entity has the entitlement to regulate? How can we deal with the monopoly of big corporation or any unwanted intervention of the state, or it is inevitably controversial and better than nothing?</p> <p>It appeals to me that the entitlement of drafting and mapping code of ethics even indicate some sort of sovereign and power relation in the field?</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
21,978-0-9952408-0-3,Society for Learning Analytics Research,EN,,"<p>The article argues that, while in the EDM field many of the researchers adopt predictive models and aspires to reach prediction accuracy, the field could benefit a lot in adopting explanatory models, in improving learning outcomes.</p> <p>Considering interpretability and actionability in producing explanatory models is important.</p> <p>Features of explanatory models, including starting with 'clean' independent variables, dependent variable maps to a well-defined construct, and is characterized by fewer estimated parameters.</p> <p>&nbsp;</p>",,
22,,,,,"<p>The article presented some of the key characteristics that a good graph should have, in the example of research regarding measles. When we present graphs, while we putting our faith in the graph for it to tell the story, we do need to be mindful to pick the perspective, add narration, to let the graph tell a good story in a clear way, if not comprehensive. It is just like human telling story, it is good to have a anecdotes to boost audience' interest but it needs to be a good anecdote, so does the graph, it should be relevant, clear, informative. We need to be clear of the purpose of having this graph, then to optimize the usage of it.</p> <p>###</p> <p>The shift in making graphs. It is said in the article that there is shift from making graphs beautiful to making it clear, which is really good in the sense that graph is at the end of day, an instrument complementing to help better elaborate the arguments/facts which may be hard for audience to understand if they cannot visualize it, so it should be informative and clear at the first place.</p> <p>&nbsp;</p>",,
23,,,,,"<p>With an increasing use of data in both researches and practices, data display is not just for 'serious' usage and it is have been widely adopted in all kinds of scenarios and contexts. Though the article present the key characteristics we need to remember in a sarcastic tone of instructing how to make bad data display, yet essentially we want to have informative, clearly presented, relevant, easy-to-comprehend, accurate, data display.</p> <p>&nbsp;</p> <p>###</p> <p>""Chartjunk"" is a very interesting topic/issue raised in this article, saying that if we don't have many to be included in graph, instead of having our audience suspecting we don't really have much to show, we can fill the graph with nondata figurations, if we want to present data badly.</p> <p>This is something we really need to avoid, particularly in the sense that it is the time everyone talks about the data, and knows a little bit more about what to expect from a graph, than before. Graph is no longer an ornament, but actual information.</p>",,
24,,,,,"<p>Clash or Synergy</p> <p>""...our difficulties with some popular visualizations arose from an insufficient understanding—by ourselves and others—of the multiplicity of goals involved in data display.""</p> <p>This articles, after pointing out many of the Infovis not really abiding the statistical rules, also states that many of the times it is not really a clash but reflecting a different needs of constructor.</p> <p>While statisticians aim to provide accurate result for readers to draw conclusions, the data scientists more on the side of using infographics to attract the readers. Since both approach have their rationale and value, so there isn't necessarily clash of interests between the two, and synergy between the two could be expected to better exploit the value of each of them.</p> <p>&nbsp;</p> <p>###</p> <p>As the article pointed that, many of the times, the statistics criticize the infovis not being statistically accurate/ acceptable, without communicating or trying to understand the incentives behind of having infovis.</p> <p>It is not really a question of who is right or wrong, but more of which is more appropriate in what kind of use? and both sides can benefit a lot of from understanding each other, and then we are back to the question that what is the visualization for and what we intends our audience to get out from that specific graph? Once we have that clear, we will be in a better place, balancing between statistical relevance and infovis' charisma.</p> <p>&nbsp;</p> <p>&nbsp;</p>",,
25,,,,Blog,"<p>The criteria of criticism, as what the author think, should comes from three perspectives, good question, good data, good visual. And the author presents different examples regarding fulfilling or failing to fulfill the three criteria. Among which, I do think the Type QD is the easies to address, having good question, good and relevant data, so only the visual part need to be addressed. Then most of the case in real world, I FEEL that this is the least scenario happening, most of the difficult cases are, either the question being left out to address, or no good or relevant data is available. And these two questions, are not just about good/bad visualization, much more beyond that. What can we do to best address that?</p> <p>&nbsp;</p> <p>###</p> <p>The author states three criteria, Q- question, D- data, V- visual, for elaboration what is a good visualization, and what is not. While good examples are presented, I do feel it is lacking real value in practice. Since most of the researchers, data scientists all want to to fulfill the three criteria, but many of the cases, especially for bad or no question, and bad data, it is more about research (project) design, and the availability of the data, which lies far beyond the visualization part.</p>",,
