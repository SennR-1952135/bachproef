{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7858314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import re #for camel case conversion\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import ParquetFile\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "# from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "#     convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7b0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case(s):\n",
    "  s = re.sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\")\n",
    "  return ''.join([s[0].lower(), s[1:]])\n",
    "\n",
    "def sherlock_case(s):\n",
    "    s = re.sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\")\n",
    "    s = ''.join([s[0].lower(), s[1:]])\n",
    "    s = ''.join(map(lambda x: x if x.islower() else \" \"+x, s))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = '../data/data/raw'\n",
    "path_processed = '../data/data/processed'\n",
    "path_out_true_types = '../../combined/results/true_types'\n",
    "path_out_predictions = '../../combined/results/predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f33b0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_values = pd.read_parquet(join(path_raw, \"test_values.parquet\"))\n",
    "train_values = pd.read_parquet(join(path_raw, \"train_values.parquet\"))\n",
    "validation_values = pd.read_parquet(join(path_raw, \"val_values.parquet\"))\n",
    "\n",
    "# test_labels = pd.read_parquet(join(path_raw, \"test_labels.parquet\"))\n",
    "# train_labels = pd.read_parquet(join(path_raw, \"train_labels.parquet\"))\n",
    "validation_labels = pd.read_parquet(join(path_raw, \"val_labels.parquet\"))\n",
    "\n",
    "# test_processed = pd.read_parquet(join(path_processed, \"test.parquet\"))\n",
    "# train_processed = pd.read_parquet(join(path_processed, \"train.parquet\"))\n",
    "validation_processed = pd.read_parquet(join(path_processed, \"validation.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c33f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = pd.concat([test_values, train_values, validation_values], ignore_index = True, axis = 0)\n",
    "# labels = pd.concat([test_labels, train_labels, validation_labels], ignore_index = True, axis = 0)\n",
    "# processed = pd.concat([test_processed, train_processed, validation_processed], ignore_index = True, axis = 0)\n",
    "labels = validation_labels\n",
    "processed = validation_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691cc649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137353\n",
      "137353\n"
     ]
    }
   ],
   "source": [
    "# values\n",
    "# print(type(values)) \n",
    "# print(len(values))\n",
    "print(len(processed))\n",
    "# labels.head(10)\n",
    "print(len(labels))\n",
    "# Write out true types for dataset\n",
    "labels.to_parquet(join(path_out_true_types, \"sherlock_validation.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967a86e",
   "metadata": {},
   "source": [
    "### Sherlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f5803c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0516 22:57:23.683773 140402237970240 deprecation.py:506] From /home/senn/virtualenvs/sherlock/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0516 22:57:23.684644 140402237970240 deprecation.py:506] From /home/senn/virtualenvs/sherlock/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0516 22:57:23.686903 140402237970240 deprecation.py:506] From /home/senn/virtualenvs/sherlock/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0516 22:57:23.701927 140402237970240 deprecation.py:506] From /home/senn/virtualenvs/sherlock/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2022-05-16 22:57:24.021201: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-05-16 22:57:24.037317: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193910000 Hz\n",
      "2022-05-16 22:57:24.038295: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e2729aaf70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-16 22:57:24.038317: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = SherlockModel();\n",
    "# model.model_files_directory = '../../../sherlock/model_files'\n",
    "model.initialize_model_from_json(with_weights=True, model_id=\"sherlock\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826d07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(processed, \"sherlock\")\n",
    "predicted_labels = pd.DataFrame(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d147f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.columns = ['type']\n",
    "predicted_labels.to_parquet(join(path_out_predictions, \"sherlock_sherlock_validation.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d9aa9",
   "metadata": {},
   "source": [
    "### Sato, NOT USABLE SEE SATO FOLDER FOR EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffd1603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['address', 'affiliate', 'affiliation', 'age', 'album', 'area', 'artist', 'birthDate', 'birthPlace', 'brand', 'capacity', 'category', 'city', 'class', 'classification', 'club', 'code', 'collection', 'command', 'company', 'component', 'continent', 'country', 'county', 'creator', 'credit', 'currency', 'day', 'depth', 'description', 'director', 'duration', 'education', 'elevation', 'family', 'fileSize', 'format', 'gender', 'genre', 'grades', 'isbn', 'industry', 'jockey', 'language', 'location', 'manufacturer', 'name', 'nationality', 'notes', 'operator', 'order', 'organisation', 'origin', 'owner', 'person', 'plays', 'position', 'product', 'publisher', 'range', 'rank', 'ranking', 'region', 'religion', 'requirement', 'result', 'sales', 'service', 'sex', 'species', 'state', 'status', 'symbol', 'team', 'teamName', 'type', 'weight', 'year']\n"
     ]
    }
   ],
   "source": [
    "# TYPENAME = os.environ['TYPENAME']\n",
    "# valid_types = get_valid_types(TYPENAME)\n",
    "# print(valid_types)\n",
    "# label_enc = LabelEncoder()\n",
    "# label_enc.fit(valid_types)\n",
    "\n",
    "# MAX_COL_COUNT = 10\n",
    "# topic_dim = 400\n",
    "# pre_trained_loc = './pretrained_sato'\n",
    "# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # print(\"PyTorch device={}\".format(device))\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093a0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_group_cols = {}\n",
    "# sherlock_feature_groups = ['char', 'word', 'par', 'rest']\n",
    "# for f_g in sherlock_feature_groups:\n",
    "#     feature_group_cols[f_g] = list(pd.read_csv(join(os.environ['BASEPATH'],\n",
    "#                                           'configs', 'feature_groups', \n",
    "#                                           \"{}_col.tsv\".format(f_g)),\n",
    "#                                            sep='\\t', header=None, \n",
    "#                                            index_col=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8179e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_vec = lambda x: np.pad(x, (0, topic_dim - len(x)),\n",
    "#                     'constant',\n",
    "#                     constant_values=(0.0, 1/topic_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb98491",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pretrained_sato/model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aa82cd370b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.load_state_dict(torch.load(join(pre_trained_loc, 'model.pt'), map_location=device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloaded_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_trained_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col_classifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CRF_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/combined/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pretrained_sato/model.pt'"
     ]
    }
   ],
   "source": [
    "# classifier = models_sherlock.build_sherlock(sherlock_feature_groups, num_classes=len(valid_types), topic_dim=topic_dim, dropout_ratio=0.35)\n",
    "# #classifier.load_state_dict(torch.load(join(pre_trained_loc, 'sherlock_None.pt'), map_location=device))\n",
    "# model = CRF(len(valid_types) , batch_first=True).to(device)\n",
    "# #model.load_state_dict(torch.load(join(pre_trained_loc, 'model.pt'), map_location=device))\n",
    "\n",
    "# loaded_params = torch.load(join(pre_trained_loc, 'model.pt'), map_location=device)\n",
    "# classifier.load_state_dict(loaded_params['col_classifier'])\n",
    "# model.load_state_dict(loaded_params['CRF_model'])\n",
    "\n",
    "# classifier.eval()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract(df):\n",
    "\n",
    "#     df_dic = {'df':df, 'locator':'None', 'dataset_id':'None'}\n",
    "#     feature_dic = {}\n",
    "#     n = df.shape[1]\n",
    "\n",
    "#     # topic vectors\n",
    "#     topic_features = extract_topic_features(df_dic)\n",
    "#     topic_vec = pad_vec(topic_features.loc[0,'table_topic'])\n",
    "#     feature_dic['topic'] = torch.FloatTensor(np.vstack((np.tile(topic_vec,(n,1)), np.zeros((MAX_COL_COUNT - n, topic_dim)))))\n",
    "\n",
    "\n",
    "#     # sherlock vectors\n",
    "#     sherlock_features = extract_sherlock_features(df_dic)\n",
    "#     print(sherlock_features.head())\n",
    "#     for f_g in feature_group_cols:\n",
    "#         temp = sherlock_features[feature_group_cols[f_g]].to_numpy()\n",
    "#         temp = np.vstack((temp, np.zeros((MAX_COL_COUNT - n, temp.shape[1])))).astype('float')\n",
    "#         temp = np.nan_to_num(temp)\n",
    "#         feature_dic[f_g] = torch.FloatTensor(temp)\n",
    "\n",
    "#     # dictionary of features, labels, masks\n",
    "#     return feature_dic, np.zeros(MAX_COL_COUNT), torch.tensor([1]*n + [0]*(MAX_COL_COUNT-n), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93250f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(df):\n",
    "\n",
    "#     feature_dic, labels, mask = extract(df)\n",
    "\n",
    "#     emissions = classifier(feature_dic).view(1, MAX_COL_COUNT, -1)\n",
    "#     mask = mask.view(1, MAX_COL_COUNT)\n",
    "#     pred = model.decode(emissions, mask)[0]\n",
    "\n",
    "#     return label_enc.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39478f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = pd.DataFrame(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, col in enumerate(values['values']):\n",
    "#     col = ast.literal_eval(col)\n",
    "#     df = pd.DataFrame(columns=['col'], data=col)\n",
    "#     prediction = evaluate(df)\n",
    "#     predicted_labels['type'].append(prediction)\n",
    "#     if(idx % 1000 == 0):\n",
    "#         break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
